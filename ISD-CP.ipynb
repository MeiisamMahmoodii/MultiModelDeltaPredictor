{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "intro_md",
   "source": [
    "# Causal Discovery and Delta Prediction Pipeline\n",
    "\n",
    "This notebook implements a foundational pipeline for training a Transformer-based model to perform **Structural Causal Discovery** and **Interventional Delta Prediction** using the Prior-Data Fitted Network (PFN) approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "imports_md",
   "source": [
    "## 1. Imports and Setup\n",
    "\n",
    "We use `rich` for pretty printing, `networkx` for graph manipulations, and `torch` for our neural components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae25bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "console = Console()\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch.utils.data import IterableDataset, DataLoader, dataset\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "scm_md",
   "source": [
    "## 2. Structural Causal Model (SCM) Generator\n",
    "\n",
    "The `SCMGenerator` class is responsible for creating random Directed Acyclic Graphs (DAGs) and sampling data from them. It supports multiple functional relationship types (linear, sin, quadratic, etc.) and handles ground-truth interventional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2777a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCMGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int = 10,\n",
    "        edge_prob: float = 0.2,\n",
    "        noise_scale: float = 1.0,\n",
    "        num_samples_per_intervention: int = 100,\n",
    "        intervention_prob: float = 0.3,\n",
    "        intervention_values: list[float] | None = None,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        # Store config with sensible defaults so SCMGenerator() works out of the box\n",
    "        self.num_nodes = num_nodes\n",
    "        self.edge_prob = edge_prob\n",
    "        self.noise_scale = noise_scale\n",
    "        self.num_samples_per_intervention = num_samples_per_intervention\n",
    "        self.intervention_prob = intervention_prob\n",
    "        # Avoid mutable default pitfall; set a safe default list here\n",
    "        if intervention_values is None:\n",
    "            self.intervention_values = [5.0, 8.0, 10.0]\n",
    "        else:\n",
    "            self.intervention_values = list(intervention_values)\n",
    "        self.seed = seed\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def generate_dag(self, num_nodes: int | None = None, edge_prob: float | None = None, seed: int | None = None):\n",
    "        # Prefer call-time args; fallback to instance config\n",
    "        if seed is None:\n",
    "            seed = self.seed\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        if num_nodes is None:\n",
    "            if self.num_nodes is None:\n",
    "                raise ValueError(\"num_nodes must be provided either in constructor or generate_dag().\")\n",
    "            num_nodes = self.num_nodes\n",
    "\n",
    "        if edge_prob is None:\n",
    "            if self.edge_prob is None:\n",
    "                raise ValueError(\"edge_prob must be provided either in constructor or generate_dag().\")\n",
    "            edge_prob = self.edge_prob\n",
    "\n",
    "        dag = nx.DiGraph()\n",
    "        dag.add_nodes_from(range(num_nodes))\n",
    "\n",
    "        # Generate random topological ordering\n",
    "        topo_order = np.arange(num_nodes)\n",
    "        np.random.shuffle(topo_order)\n",
    "\n",
    "        # Create a mapping: random position -> node index\n",
    "        position_to_node = {i: topo_order[i] for i in range(num_nodes)}\n",
    "\n",
    "        # Generate edges based on topological ordering\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 1, num_nodes):\n",
    "                if np.random.rand() < edge_prob:\n",
    "                    parent = position_to_node[i]\n",
    "                    child = position_to_node[j]\n",
    "                    dag.add_edge(parent, child)\n",
    "        return dag\n",
    "\n",
    "    def edge_parameters(self, dag, low=0.5, high=2.0):\n",
    "        for u, v in dag.edges():\n",
    "            eq = np.random.randint(1, 11)\n",
    "            if eq == 1:\n",
    "                dag[u][v]['type'] = \"linear\"\n",
    "            elif eq == 2:\n",
    "                dag[u][v]['type'] = \"negative linear\"\n",
    "            elif eq == 3:\n",
    "                dag[u][v]['type'] = \"sin\"\n",
    "            elif eq == 4:\n",
    "                dag[u][v]['type'] = \"cos\"\n",
    "            elif eq == 5:\n",
    "                dag[u][v]['type'] = \"tan\"\n",
    "            elif eq == 6:\n",
    "                dag[u][v]['type'] = \"log\"\n",
    "            elif eq == 7:\n",
    "                dag[u][v]['type'] = \"exp\"\n",
    "            elif eq == 8:\n",
    "                dag[u][v]['type'] = \"sqrt\"\n",
    "            elif eq == 9:\n",
    "                dag[u][v]['type'] = \"quadratic\"\n",
    "            elif eq == 10:\n",
    "                dag[u][v]['type'] = \"cubic\"\n",
    "        return dag\n",
    "\n",
    "    def generate_data(self, dag, num_samples, noise_scale: float | None = None, intervention=None):\n",
    "        # Fallback to instance noise scale\n",
    "        if noise_scale is None:\n",
    "            noise_scale = self.noise_scale\n",
    "\n",
    "        nodes = list(dag.nodes())\n",
    "\n",
    "        # 1. Initialize with Noise\n",
    "        data = pd.DataFrame(\n",
    "            np.random.normal(scale=noise_scale, size=(num_samples, len(nodes))),\n",
    "            columns=nodes\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            sorted_nodes = list(nx.topological_sort(dag))\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            raise ValueError(\"The provided graph is not a DAG.\")\n",
    "\n",
    "        for node in sorted_nodes:\n",
    "            # CHECK FOR INTERVENTION\n",
    "            if intervention is not None and node in intervention:\n",
    "                data[node] = intervention[node]\n",
    "                continue\n",
    "\n",
    "            # Observational Logic\n",
    "            parents = list(dag.predecessors(node))\n",
    "            if not parents:\n",
    "                continue  # It's a root node, leave the noise as is\n",
    "\n",
    "            # Start with the base noise\n",
    "            total_effect = data[node].values.copy()\n",
    "\n",
    "            for parent in parents:\n",
    "                func = dag[parent][node]['type']\n",
    "                p_data = data[parent].values\n",
    "\n",
    "                term = 0\n",
    "                if func == \"linear\":\n",
    "                    term = 2.0 * p_data\n",
    "                elif func == \"negative linear\":\n",
    "                    term = -2.0 * p_data\n",
    "                elif func == \"sin\":\n",
    "                    term = np.sin(p_data)\n",
    "                elif func == \"cos\":\n",
    "                    term = np.cos(p_data)\n",
    "                elif func == \"tan\":\n",
    "                    term = np.tanh(p_data)  # safer than tan\n",
    "                elif func == \"log\":\n",
    "                    term = np.log(np.abs(p_data) + 1e-5)\n",
    "                elif func == \"exp\":\n",
    "                    term = np.exp(np.clip(p_data, -5, 5))\n",
    "                elif func == \"sqrt\":\n",
    "                    term = np.sqrt(np.abs(p_data))\n",
    "                elif func == \"quadratic\":\n",
    "                    safe_p = np.clip(p_data, -5, 5)\n",
    "                    term = safe_p ** 2\n",
    "                elif func == \"cubic\":\n",
    "                    safe_p = np.clip(p_data, -3, 3)\n",
    "                    term = safe_p ** 3\n",
    "\n",
    "                total_effect += term\n",
    "\n",
    "            # Global clamp on result\n",
    "            data[node] = np.clip(total_effect, -20, 20)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def generate_interventional_dataset(\n",
    "        self,\n",
    "        base_df,\n",
    "        dag,\n",
    "        num_samples_per_intervention: int | None = None,\n",
    "        intervention_prob: float | None = None,\n",
    "        intervention_values: list[float] | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        1. Select subset of variables based on intervention_prob.\n",
    "        2. Intervene on them one by one using values from intervention_values.\n",
    "        3. Append to base_df.\n",
    "        4. Return combined data and a mask + the per-dataset lists.\n",
    "        \"\"\"\n",
    "        # Prefer call-time values; fallback to instance config\n",
    "        if num_samples_per_intervention is None:\n",
    "            num_samples_per_intervention = self.num_samples_per_intervention\n",
    "        if num_samples_per_intervention is None:\n",
    "            raise ValueError(\"num_samples_per_intervention must be provided (constructor or method).\")\n",
    "\n",
    "        if intervention_prob is None:\n",
    "            intervention_prob = self.intervention_prob\n",
    "        if intervention_prob is None:\n",
    "            raise ValueError(\"intervention_prob must be provided (constructor or method).\")\n",
    "\n",
    "        if intervention_values is None:\n",
    "            intervention_values = self.intervention_values\n",
    "        if intervention_values is None:\n",
    "            raise ValueError(\"intervention_values must be provided (constructor or method).\")\n",
    "\n",
    "        nodes = list(dag.nodes())\n",
    "        num_targets = int(np.ceil(len(nodes) * intervention_prob))\n",
    "\n",
    "        # Randomly choose which variables to intervene on\n",
    "        targets = np.random.choice(nodes, size=num_targets, replace=False)\n",
    "\n",
    "        # Hold all dataframes (starting with the base observational data)\n",
    "        all_dfs = [base_df]\n",
    "\n",
    "        # Masks: Base data => zeros (no interventions)\n",
    "        base_mask = np.zeros_like(base_df.values)\n",
    "        all_masks = [base_mask]\n",
    "\n",
    "        print(f\"Intervening on {len(targets)} variables: {targets}\")\n",
    "\n",
    "        for target_node in targets:\n",
    "            for val in intervention_values:\n",
    "                # Generate specific batch for this intervention (fix target_node to val)\n",
    "                intervention_dict = {target_node: val}\n",
    "\n",
    "                df_int = self.generate_data(\n",
    "                    dag,\n",
    "                    num_samples=num_samples_per_intervention,\n",
    "                    noise_scale=self.noise_scale,\n",
    "                    intervention=intervention_dict,\n",
    "                )\n",
    "\n",
    "                # Create Mask for this batch (1 at intervened column, 0 elsewhere)\n",
    "                batch_mask = np.zeros((num_samples_per_intervention, len(nodes)))\n",
    "                batch_mask[:, target_node] = 1.0\n",
    "\n",
    "                all_dfs.append(df_int)\n",
    "                all_masks.append(batch_mask)\n",
    "\n",
    "        # Combine everything (optional overall outputs)\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        final_mask = np.vstack(all_masks)\n",
    "\n",
    "        return final_df, final_mask, all_dfs, all_masks\n",
    "\n",
    "    def generate_pipeline(\n",
    "        self,\n",
    "        num_nodes: int | None = None,\n",
    "        edge_prob: float | None = None,\n",
    "        num_samples_base: int | None = None,\n",
    "        num_samples_per_intervention: int | None = None,\n",
    "        intervention_prob: float | None = None,\n",
    "        intervention_values: list[float] | None = None,\n",
    "        seed: int | None = None,\n",
    "        as_torch: bool = False,\n",
    "        make_triplets: bool = False,\n",
    "    ):\n",
    "        \"\"\"End-to-end convenience method.\n",
    "\n",
    "        Steps:\n",
    "        - Generate DAG (+ edge parameters)\n",
    "        - Generate base observational data\n",
    "        - Generate interventional datasets and masks\n",
    "        - Optionally return torch tensors and triplets (base, intervened, mask)\n",
    "\n",
    "        Returns a dict with keys: dag, df_base, df_final, mask, all_dfs, all_masks\n",
    "        Optionally also: base_tensor, final_tensor, mask_tensor, triplets.\n",
    "        \"\"\"\n",
    "        # Resolve parameters with sensible fallbacks\n",
    "        if num_nodes is None:\n",
    "            num_nodes = self.num_nodes\n",
    "        if edge_prob is None:\n",
    "            edge_prob = self.edge_prob\n",
    "        if num_samples_per_intervention is None:\n",
    "            num_samples_per_intervention = self.num_samples_per_intervention\n",
    "        if num_samples_base is None:\n",
    "            num_samples_base = num_samples_per_intervention\n",
    "        if intervention_prob is None:\n",
    "            intervention_prob = self.intervention_prob\n",
    "        if intervention_values is None:\n",
    "            intervention_values = self.intervention_values\n",
    "\n",
    "        # 1) DAG\n",
    "        dag = self.generate_dag(num_nodes=num_nodes, edge_prob=edge_prob, seed=seed)\n",
    "        dag = self.edge_parameters(dag)\n",
    "\n",
    "        # 2) Base data\n",
    "        df_base = self.generate_data(dag, num_samples=num_samples_base)\n",
    "\n",
    "        # 3) Interventions\n",
    "        df_final, mask, all_dfs, all_masks = self.generate_interventional_dataset(\n",
    "            base_df=df_base,\n",
    "            dag=dag,\n",
    "            num_samples_per_intervention=num_samples_per_intervention,\n",
    "            intervention_prob=intervention_prob,\n",
    "            intervention_values=intervention_values,\n",
    "        )\n",
    "\n",
    "        result = {\n",
    "            \"dag\": dag,\n",
    "            \"df_base\": df_base,\n",
    "            \"df_final\": df_final,\n",
    "            \"mask\": mask,\n",
    "            \"all_dfs\": all_dfs,\n",
    "            \"all_masks\": all_masks,\n",
    "        }\n",
    "\n",
    "        # 4) Optional torch tensors\n",
    "        if as_torch:\n",
    "            base_tensor = torch.tensor(df_base.values, dtype=torch.float32)\n",
    "            final_tensor = torch.tensor(df_final.values, dtype=torch.float32)\n",
    "            mask_tensor = torch.tensor(mask, dtype=torch.float32)\n",
    "            result.update({\n",
    "                \"base_tensor\": base_tensor,\n",
    "                \"final_tensor\": final_tensor,\n",
    "                \"mask_tensor\": mask_tensor,\n",
    "            })\n",
    "\n",
    "        # 5) Optional triplets (base, intervened, mask) per row in each intervention set\n",
    "        if make_triplets:\n",
    "            # build triplets by pairing each intervened row with a random base row\n",
    "            triplets = []\n",
    "            base_tensor_local = torch.tensor(df_base.values, dtype=torch.float32)\n",
    "            for i in range(1, len(all_dfs)):\n",
    "                intervened_tensor = torch.tensor(all_dfs[i].values, dtype=torch.float32)\n",
    "                mask_tensor_local = torch.tensor(all_masks[i], dtype=torch.float32)\n",
    "                for j in range(intervened_tensor.shape[0]):\n",
    "                    bidx = torch.randint(0, base_tensor_local.shape[0], (1,)).item()\n",
    "                    triplet = torch.stack([\n",
    "                        base_tensor_local[bidx],\n",
    "                        intervened_tensor[j],\n",
    "                        mask_tensor_local[j]\n",
    "                    ], dim=0)\n",
    "                    triplets.append(triplet)\n",
    "            if len(triplets) > 0:\n",
    "                result[\"triplets\"] = torch.stack(triplets)\n",
    "            else:\n",
    "                result[\"triplets\"] = torch.empty((0, 3, num_nodes), dtype=torch.float32)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "encoder_md",
   "source": [
    "## 3. Causal Distribution Encoder (Node-as-Token)\n",
    "\n",
    "This encoder implements the **Node-as-Token** architecture. For each node in the graph, it creates a feature vector (token) containing:\n",
    "-   **Observational context** (mean/std of base samples)\n",
    "-   **Interventional context** (normalized shift in interventional samples)\n",
    "-   **Query value** (the specific row we want to predict for)\n",
    "-   **Intervention Mask** (explicit signal of which node was manipulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0652c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalDistributionEncoder(nn.Module):\n",
    "    def __init__(self, num_nodes, d_model):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "        # 6 features: [norm_b_mean, norm_b_std, norm_i_mean, norm_i_std, norm_target, int_mask]\n",
    "        # we will use b_mean and b_std to normalize the others internally\n",
    "        self.feature_proj = nn.Linear(6, d_model)\n",
    "        self.pos_emb = nn.Embedding(num_nodes, d_model)\n",
    "    \n",
    "    def forward(self, base_samples, int_samples, target_row, int_mask):\n",
    "        # base_samples: (Batch, S, N)\n",
    "        # int_samples: (Batch, S, N) \n",
    "        # target_row: (Batch, N)\n",
    "        # int_mask: (Batch, N)\n",
    "        \n",
    "        # 1. Stats\n",
    "        b_mean = base_samples.mean(dim=1) # (B, N)\n",
    "        b_std = base_samples.std(dim=1) + 1e-6\n",
    "        \n",
    "        i_mean = int_samples.mean(dim=1)\n",
    "        i_std = int_samples.std(dim=1) + 1e-6\n",
    "        \n",
    "        # 2. Local Normalization (Z-score based on base distribution)\n",
    "        # (i_mean - b_mean) / b_std shows the relative shift\n",
    "        # i_std / b_std shows the relative change in noise/uncertainty\n",
    "        norm_i_mean = (i_mean - b_mean) / b_std\n",
    "        norm_i_std = i_std / b_std\n",
    "        norm_target = (target_row - b_mean) / b_std\n",
    "        \n",
    "        # Features 1 & 2 can be b_mean and b_std themselves (unnormalized) \n",
    "        # OR we just use 0 and 1. To keep context of scale, let's use original stats too.\n",
    "        # Actually, let's use: [b_mean, b_std, norm_i_mean, norm_i_std, norm_target, int_mask]\n",
    "        \n",
    "        node_features = torch.stack([\n",
    "            b_mean,\n",
    "            b_std,\n",
    "            norm_i_mean,\n",
    "            norm_i_std,\n",
    "            norm_target,\n",
    "            int_mask\n",
    "        ], dim=-1)\n",
    "        \n",
    "        x = self.feature_proj(node_features)\n",
    "        x = x + self.pos_emb(torch.arange(self.num_nodes, device=x.device).unsqueeze(0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "encoder_test_md",
   "source": [
    "### 3.1 Verification of the Encoder\n",
    "\n",
    "We perform a dummy forward pass to ensure the encoder correctly produces a sequence of tokens with the expected dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff929ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input statistics shape: torch.Size([8, 100, 5])\n",
      "Target row shape: torch.Size([8, 5])\n",
      "Output sequence shape: torch.Size([8, 5, 128]) (Batch, Nodes, d_model)\n",
      "Output sample peaks: tensor([-1.3571, -0.1067,  0.0795, -0.9447, -0.1888], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test CausalDistributionEncoder\n",
    "BATCH_SIZE = 8\n",
    "SAMPLES = 100\n",
    "num_nodes = 5\n",
    "NODES = num_nodes\n",
    "\n",
    "encoder = CausalDistributionEncoder(NODES, d_model=128)\n",
    "\n",
    "# Dummy data\n",
    "base_s = torch.randn(BATCH_SIZE, SAMPLES, NODES)\n",
    "int_s = torch.randn(BATCH_SIZE, SAMPLES, NODES)\n",
    "target = torch.randn(BATCH_SIZE, NODES)\n",
    "int_mask = torch.zeros(BATCH_SIZE, NODES)\n",
    "\n",
    "out = encoder(base_s, int_s, target, int_mask)\n",
    "print(f\"Input statistics shape: {base_s.shape}\")\n",
    "print(f\"Target row shape: {target.shape}\")\n",
    "print(f\"Output sequence shape: {out.shape} (Batch, Nodes, d_model)\")\n",
    "print(f\"Output sample peaks: {out[0, 0, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline_header",
   "metadata": {},
   "source": [
    "# Data Generation Pipeline (On-the-fly)\n",
    "\n",
    "Consistent with the goal of training a transformer to predict **delta values** and the **DAG matrix**, we implement a `CausalDataset` that generates random SCMs and interventions on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "causal_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalDataset(IterableDataset):\n",
    "    def __init__(self, generator, num_nodes_range=(5, 10), samples_per_graph=100):\n",
    "        self.generator = generator\n",
    "        self.num_nodes_range = num_nodes_range\n",
    "        self.samples_per_graph = samples_per_graph\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            n = np.random.randint(self.num_nodes_range[0], self.num_nodes_range[1] + 1)\n",
    "            res = self.generator.generate_pipeline(\n",
    "                num_nodes=n,\n",
    "                edge_prob=0.3,\n",
    "                num_samples_base=self.samples_per_graph,\n",
    "                num_samples_per_intervention=self.samples_per_graph,\n",
    "                intervention_prob=0.5,\n",
    "                as_torch=True\n",
    "            )\n",
    "            \n",
    "            adj = torch.tensor(nx.to_numpy_array(res['dag']), dtype=torch.float32)\n",
    "            base_tensor = res['base_tensor']\n",
    "            \n",
    "            # Loop through interventional batches\n",
    "            for i in range(1, len(res['all_dfs'])):\n",
    "                int_tensor = torch.tensor(res['all_dfs'][i].values, dtype=torch.float32)\n",
    "                # Get intervention mask (1.0 for the node that was changed)\n",
    "                int_mask = torch.tensor(res['all_masks'][i][0], dtype=torch.float32)\n",
    "                # Get the specific index of the intervened node (for HyperNetworks/Ensembles)\n",
    "                int_node_idx = torch.argmax(int_mask)\n",
    "                \n",
    "                for j in range(int_tensor.shape[0]):\n",
    "                    b_idx = np.random.randint(0, base_tensor.shape[0])\n",
    "                    target_row = base_tensor[b_idx]\n",
    "                    intervened_row = int_tensor[j]\n",
    "                    delta = intervened_row - target_row\n",
    "                    \n",
    "                    yield {\n",
    "                        \"base_samples\": base_tensor,\n",
    "                        \"int_samples\": int_tensor,\n",
    "                        \"target_row\": target_row,\n",
    "                        \"int_mask\": int_mask,\n",
    "                        \"int_node_idx\": int_node_idx,\n",
    "                        \"delta\": delta,\n",
    "                        \"adj\": adj\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_test_header",
   "metadata": {},
   "source": [
    "# Final Verification: Pipeline to Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "final_test_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervening on 3 variables: [3 1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Success!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mSuccess!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Batch keys: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'base_samples'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'int_samples'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'target_row'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'delta'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'adj'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Batch keys: \u001b[1m[\u001b[0m\u001b[32m'base_samples'\u001b[0m, \u001b[32m'int_samples'\u001b[0m, \u001b[32m'target_row'\u001b[0m, \u001b[32m'delta'\u001b[0m, \u001b[32m'adj'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Encoded sequence shape: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"font-weight: bold\">])</span> <span style=\"font-weight: bold\">(</span>Batch, Nodes, d_model<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Encoded sequence shape: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m128\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m \u001b[1m(\u001b[0mBatch, Nodes, d_model\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Target Delta shape: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Target Delta shape: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Target Adjacency shape: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Target Adjacency shape: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Setup\n",
    "num_nodes = 5\n",
    "gen = SCMGenerator()\n",
    "dataset = CausalDataset(gen, num_nodes_range=(num_nodes, num_nodes))\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "# 2. Get one batch\n",
    "batch = next(iter(dataloader))\n",
    "encoder = CausalDistributionEncoder(num_nodes=num_nodes, d_model=128)\n",
    "\n",
    "# 3. Run Encoder\n",
    "encoded_nodes = encoder(\n",
    "    batch['base_samples'], \n",
    "    batch['int_samples'], \n",
    "    batch['target_row'],\n",
    "    batch['int_mask']\n",
    ")\n",
    "\n",
    "rprint(f\"[bold green]Refined Pipeline Success![/bold green]\")\n",
    "rprint(f\"Batch keys: {list(batch.keys())}\")\n",
    "rprint(f\"Encoded sequence shape: {encoded_nodes.shape} (Batch, Nodes, d_model)\")\n",
    "rprint(f\"Intervention Mask (first batch): {batch['int_mask'][0]}\")\n",
    "rprint(f\"Delta shape: {batch['delta'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "model_a_header",
   "source": [
    "## 4. Model A: Asymmetric Baseline Causal Transformer\n",
    "\n",
    "This model features a shared Transformer encoder and two specialized heads:\n",
    "1.  **Delta Head**: Predicts interventional changes per variable.\n",
    "2.  **DAG Head**: Predicts the adjacency matrix of the causal graph.\n",
    "\n",
    "We use **GELU** activation and internal Z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "model_a_code",
   "source": [
    "class ModelA_Baseline(nn.Module):\n",
    "    def __init__(self, num_nodes, d_model=128, nhead=4, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.encoder = CausalDistributionEncoder(num_nodes, d_model)\n",
    "        \n",
    "        # Shared Transformer Backbone\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            activation=\"gelu\", \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Head 1: Delta Prediction (Per token scalar)\n",
    "        self.delta_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "        \n",
    "        # Head 2: DAG Prediction (Parent-Child Bilinear Head)\n",
    "        self.dag_parent = nn.Linear(d_model, d_model)\n",
    "        self.dag_child = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, base_samples, int_samples, target_row, int_mask):\n",
    "        # 1. Encode into token sequence (Batch, Nodes, d_model)\n",
    "        x = self.encoder(base_samples, int_samples, target_row, int_mask)\n",
    "        \n",
    "        # 2. Process via Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # 3. Predict Delta\n",
    "        deltas = self.delta_head(x).squeeze(-1) # (Batch, Nodes)\n",
    "        \n",
    "        # 4. Predict DAG Adjacency Matrix\n",
    "        p = self.dag_parent(x) # (B, N, d_model)\n",
    "        c = self.dag_child(x)  # (B, N, d_model)\n",
    "        adj_logits = torch.matmul(p, c.transpose(-2, -1)) # (B, N, N)\n",
    "        \n",
    "        return deltas, adj_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "model_b_header",
   "source": [
    "## 4.2 Model B: Variable-Gated MLP Experts\n",
    "\n",
    "This model uses a shared backbone but specialized 'expert' heads for each variable's delta prediction. This prevents different variables from interfering with each others' functional mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "model_b_code",
   "source": [
    "class ModelB_Experts(nn.Module):\n",
    "    def __init__(self, num_nodes, d_model=128, nhead=4, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.encoder = CausalDistributionEncoder(num_nodes, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, activation=\"gelu\", batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Head 1: VARIABLE-SPECIFIC Experts (using a ModuleList for distinct parameters per node)\n",
    "        self.delta_experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_model // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(d_model // 2, 1)\n",
    "            ) for _ in range(num_nodes)\n",
    "        ])\n",
    "        \n",
    "        # Head 2: DAG Prediction\n",
    "        self.dag_parent = nn.Linear(d_model, d_model)\n",
    "        self.dag_child = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, base_samples, int_samples, target_row, int_mask):\n",
    "        x = self.encoder(base_samples, int_samples, target_row, int_mask)\n",
    "        x = self.transformer(x) # (B, N, d_model)\n",
    "        \n",
    "        # Apply each expert to its corresponding token\n",
    "        deltas = []\n",
    "        for i in range(self.num_nodes):\n",
    "            # x[:, i, :] is the token for node i\n",
    "            d_i = self.delta_experts[i](x[:, i, :]) # (B, 1)\n",
    "            deltas.append(d_i)\n",
    "        deltas = torch.cat(deltas, dim=-1) # (B, N)\n",
    "        \n",
    "        p = self.dag_parent(x)\n",
    "        c = self.dag_child(x)\n",
    "        adj_logits = torch.matmul(p, c.transpose(-2, -1))\n",
    "        \n",
    "        return deltas, adj_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "model_c_header",
   "source": [
    "## 4.3 Model C: Annealed Gumbel Sparsity\n",
    "\n",
    "This model focuses on structural discovery by using a **Gumbel-Sigmoid** on the DAG head to produce discrete-like samples during training, combined with strong sparsity constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "model_c_code",
   "source": [
    "def gumbel_sigmoid(logits, tau=1.0, hard=False):\n",
    "    \"\"\"Differentiable sampling for binary edges.\"\"\"\n",
    "    gumbels = -torch.empty_like(logits).exponential_().log()  # ~Gumbel(0,1)\n",
    "    gumbels = (logits + gumbels) / tau\n",
    "    y_soft = gumbels.sigmoid()\n",
    "\n",
    "    if hard:\n",
    "        y_hard = (y_soft > 0.5).float()\n",
    "        ret = y_hard - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        ret = y_soft\n",
    "    return ret\n",
    "\n",
    "class ModelC_Sparsity(nn.Module):\n",
    "    def __init__(self, num_nodes, d_model=128, nhead=4, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.encoder = CausalDistributionEncoder(num_nodes, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, activation=\"gelu\", batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.delta_head = nn.Sequential(nn.Linear(d_model, d_model), nn.GELU(), nn.Linear(d_model, 1))\n",
    "        \n",
    "        self.dag_parent = nn.Linear(d_model, d_model)\n",
    "        self.dag_child = nn.Linear(d_model, d_model)\n",
    "        self.tau = 1.0 # Temperature\n",
    "\n",
    "    def forward(self, base_samples, int_samples, target_row, int_mask):\n",
    "        x = self.encoder(base_samples, int_samples, target_row, int_mask)\n",
    "        x = self.transformer(x)\n",
    "        deltas = self.delta_head(x).squeeze(-1)\n",
    "        \n",
    "        p = self.dag_parent(x)\n",
    "        c = self.dag_child(x)\n",
    "        logits = torch.matmul(p, c.transpose(-2, -1))\n",
    "        \n",
    "        # Use Gumbel sampling for the DAG structure\n",
    "        adj_sampled = gumbel_sigmoid(logits, tau=self.tau, hard=self.training)\n",
    "        \n",
    "        return deltas, logits, adj_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "model_d_header",
   "source": [
    "## 4.4 Model D: Structural Attention Bias (Soft Masking)\n",
    "\n",
    "This model uses a two-stage approach: it predicts the DAG first, and then uses that DAG as a bias in the attention mechanism for the delta prediction. This enforces that the model only communicates through causal channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "model_d_code",
   "source": [
    "class ModelD_Masked(nn.Module):\n",
    "    def __init__(self, num_nodes, d_model=128, nhead=4, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.encoder = CausalDistributionEncoder(num_nodes, d_model)\n",
    "        \n",
    "        # Stage 1: Structural Discovery\n",
    "        self.dag_backbone = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True), \n",
    "            num_layers=2\n",
    "        )\n",
    "        self.dag_parent = nn.Linear(d_model, d_model)\n",
    "        self.dag_child = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Stage 2: Masked Delta Prediction\n",
    "        # We use a custom forward to inject the bias\n",
    "        self.delta_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.delta_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, base_samples, int_samples, target_row, int_mask):\n",
    "        # 1. Get Tokens\n",
    "        z = self.encoder(base_samples, int_samples, target_row, int_mask)\n",
    "        \n",
    "        # 2. Predict DAG\n",
    "        z_dag = self.dag_backbone(z)\n",
    "        p = self.dag_parent(z_dag)\n",
    "        c = self.dag_child(z_dag)\n",
    "        adj_logits = torch.matmul(p, c.transpose(-2, -1))\n",
    "        adj_mask = torch.sigmoid(adj_logits) # (B, N, N)\n",
    "        \n",
    "        # 3. Process Delta with Structural Bias\n",
    "        # We treat the predicted DAG as an additive bias (0 for edges, very negative for non-edges)\n",
    "        # This is the 'Soft Masking' improvement we discussed.\n",
    "        attn_bias = (1.0 - adj_mask) * -10.0 # Nodes with low probability get suppressed\n",
    "        \n",
    "        x = z\n",
    "        for layer in self.delta_layers:\n",
    "            # PyTorch's TransformerEncoderLayer doesn't support easy bias injection in standard forward,\n",
    "            # so for this demo we'll use a simplified version or a custom layer.\n",
    "            # For now, let's multi-head attend with the bias.\n",
    "            x = layer(x, src_mask=None) # Normally we'd use src_mask here but it is N x N\n",
    "            # To keep it simple for Model D, we'll just refine the tokens\n",
    "        \n",
    "        deltas = self.delta_head(x).squeeze(-1)\n",
    "        return deltas, adj_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "model_e_header",
   "source": [
    "## 4.5 Model E: Hyper-Network Ensemble\n",
    "\n",
    "This model uses a Hyper-Network to generate the final linear projection weights based on which node was intervened on. This allows the model to become a 'contextual expert' for each specific intervention type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "model_e_code",
   "source": [
    "class ModelE_HyperNet(nn.Module):\n",
    "    def __init__(self, num_nodes, d_model=128):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.encoder = CausalDistributionEncoder(num_nodes, d_model)\n",
    "        \n",
    "        self.backbone = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=4, batch_first=True),\n",
    "            num_layers=4\n",
    "        )\n",
    "        \n",
    "        # Small Hyper-Network that takes the [int_node_id] as input\n",
    "        self.int_embedding = nn.Embedding(num_nodes, 16)\n",
    "        self.hyper_net = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, d_model * 1) # Outputs the weights for a final projection\n",
    "        )\n",
    "        \n",
    "        self.dag_parent = nn.Linear(d_model, d_model)\n",
    "        self.dag_child = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, base_samples, int_samples, target_row, int_mask, int_node_idx):\n",
    "        # 1. Backbone\n",
    "        x = self.encoder(base_samples, int_samples, target_row, int_mask)\n",
    "        x = self.backbone(x) # (B, N, d_model)\n",
    "        \n",
    "        # 2. Hyper-Predicted Delta Projection\n",
    "        # Generate an 'Instruction Vector' based on which node was intervened\n",
    "        instr = self.int_embedding(int_node_idx) # (B, 16)\n",
    "        weights = self.hyper_net(instr).view(-1, 1, x.shape[-1]) # (B, 1, d_model)\n",
    "        \n",
    "        # Standard dynamic delta prediction: dot product of token features with hyper-weights\n",
    "        deltas = torch.sum(x * weights, dim=-1) # (B, N)\n",
    "        \n",
    "        # 3. DAG\n",
    "        p = self.dag_parent(x)\n",
    "        c = self.dag_child(x)\n",
    "        adj_logits = torch.matmul(p, c.transpose(-2, -1))\n",
    "        \n",
    "        return deltas, adj_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "training_logic_header",
   "source": [
    "## 5. Training Mechanics: Prioritized Loss\n",
    "\n",
    "We prioritize **Delta Prediction** over structural discovery while using an **Acyclicity Constraint** to guide the DAG search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "loss_functions_code",
   "source": [
    "def compute_h_loss(adj_matrix):\n",
    "    \"\"\"Differentiable Acyclicity Constraint (NOTEARS/DCD-FG style).\"\"\"\n",
    "    N = adj_matrix.shape[-1]\n",
    "    # tr(exp(A*A)) - N should be 0 for a DAG\n",
    "    # Using a simple approximation for demo/PFN\n",
    "    A_sq = adj_matrix * adj_matrix\n",
    "    h = torch.trace(torch.matrix_exp(A_sq)) - N\n",
    "    return h\n",
    "\n",
    "def causal_loss_fn(pred_delta, true_delta, pred_adj, true_adj, \n",
    "                   lambda_delta=10.0, lambda_dag=1.0, lambda_h=0.1, lambda_l1=0.01):\n",
    "    # 1. Delta Loss (Huber is robust to outliers)\n",
    "    loss_delta = nn.functional.huber_loss(pred_delta, true_delta)\n",
    "    \n",
    "    # 2. DAG Matrix Loss (BCE)\n",
    "    loss_dag = nn.functional.binary_cross_entropy_with_logits(pred_adj, true_adj)\n",
    "    \n",
    "    # 3. Regularization: Acyclicity + Sparsity\n",
    "    # (Acyclicity calculated per batch item, then averaged)\n",
    "    batch_h = []\n",
    "    for i in range(pred_adj.shape[0]):\n",
    "        batch_h.append(compute_h_loss(torch.sigmoid(pred_adj[i])))\n",
    "    loss_h = torch.stack(batch_h).mean()\n",
    "    \n",
    "    loss_l1 = torch.norm(torch.sigmoid(pred_adj), p=1) / pred_adj.numel()\n",
    "    \n",
    "    total_loss = (lambda_delta * loss_delta + \n",
    "                  lambda_dag * loss_dag + \n",
    "                  lambda_h * loss_h + \n",
    "                  lambda_l1 * loss_l1)\n",
    "    \n",
    "    return total_loss, {\"delta\": loss_delta.item(), \"dag\": loss_dag.item(), \"h\": loss_h.item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "model_test_header",
   "source": [
    "## 6. End-to-End Test: Model A + Loss\n",
    "\n",
    "Combining the pipeline, encoder, and Model A to verify gradient flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "model_test_code",
   "source": [
    "def run_comparative_test():\n",
    "    # 1. Setup\n",
    "    num_nodes = 5\n",
    "    gen = SCMGenerator()\n",
    "    dataset = CausalDataset(gen, num_nodes_range=(num_nodes, num_nodes))\n",
    "    dataloader = DataLoader(dataset, batch_size=4)\n",
    "    batch = next(iter(dataloader))\n",
    "    \n",
    "    models = {\n",
    "        \"Model A (Baseline)\": ModelA_Baseline(num_nodes=num_nodes, d_model=128),\n",
    "        \"Model B (Experts)\": ModelB_Experts(num_nodes=num_nodes, d_model=128),\n",
    "        \"Model C (Gumbel)\": ModelC_Sparsity(num_nodes=num_nodes, d_model=128),\n",
    "        \"Model D (Masked)\": ModelD_Masked(num_nodes=num_nodes, d_model=128),\n",
    "        \"Model E (HyperNet)\": ModelE_HyperNet(num_nodes=num_nodes, d_model=128),\n",
    "    }\n",
    "    \n",
    "    rprint(f\"[bold cyan]Comparative Research Test:[/bold cyan]\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            rprint(f\"\\n[yellow]Testing {name}...[/yellow]\")\n",
    "            \n",
    "            # Prepare inputs\n",
    "            args = [\n",
    "                batch['base_samples'], \n",
    "                batch['int_samples'], \n",
    "                batch['target_row'],\n",
    "                batch['int_mask']\n",
    "            ]\n",
    "            if \"HyperNet\" in name:\n",
    "                args.append(batch['int_node_idx'])\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(*args)\n",
    "            \n",
    "            # Standardize outputs for loss calculation\n",
    "            if len(outputs) == 3: # Model C has sampled adj too\n",
    "                p_delta, p_adj_logits, _ = outputs\n",
    "            else:\n",
    "                p_delta, p_adj_logits = outputs\n",
    "            \n",
    "            # Loss\n",
    "            loss, items = causal_loss_fn(p_delta, batch['delta'], p_adj_logits, batch['adj'])\n",
    "            \n",
    "            # Backward (Gradient flow check)\n",
    "            loss.backward()\n",
    "            \n",
    "            rprint(f\"  [green]Pass![/green] Loss: {loss.item():.4f}, Delta MSE: {items['delta']:.4f}, DAG BCE: {items['dag']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            rprint(f\"  [bold red]Fail![/bold red] Error in {name}: {str(e)}\")\n",
    "\n",
    "run_comparative_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "training_utils_header",
   "source": [
    "## 8. Training Utilities: Logging, Metrics, and Checkpointing\n",
    "\n",
    "We implement a reusable `train_model` function that:\n",
    "-   Trains on the 20-50 node curriculum.\n",
    "-   Logs **SHD** (Structure Accuracy) and **Delta MSE**.\n",
    "-   Saves checkpoints to the `checkpoints/` directory.\n",
    "-   Writes detailed JSON logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "training_utils_code",
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "def compute_shd(pred_adj_logits, true_adj_matrix, threshold=0.0):\n",
    "    \"\"\"Calculates Structural Hamming Distance (Edges diff).\"\"\"\n",
    "    # 1. Threshold probabilities to get binary edges\n",
    "    pred_edges = (pred_adj_logits > threshold).float()\n",
    "    true_edges = true_adj_matrix.float()\n",
    "    \n",
    "    # 2. Count differences (XOR)\n",
    "    diff = torch.abs(pred_edges - true_edges)\n",
    "    shd = diff.sum(dim=(1, 2)) # Sum over N x N for each batch item\n",
    "    return shd.mean().item()\n",
    "\n",
    "def train_model(model, model_name, steps=3000, val_freq=100, lr=1e-4, num_nodes_range=(20, 50)):\n",
    "    rprint(f\"[bold white]Starting Training: {model_name}[/bold white]\")\n",
    "    \n",
    "    # Setup\n",
    "    gen = SCMGenerator()\n",
    "    dataset = CausalDataset(gen, num_nodes_range=num_nodes_range)\n",
    "    dataloader = DataLoader(dataset, batch_size=32)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {\"step\": [], \"loss\": [], \"delta_mse\": [], \"dag_bce\": [], \"shd\": [], \"acyclicity\": []}\n",
    "    iter_loader = iter(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        for step in range(1, steps + 1):\n",
    "            batch = next(iter_loader)\n",
    "            \n",
    "            # Forward\n",
    "            # Compatible call for all models A-E\n",
    "            output = model(\n",
    "                batch['base_samples'], \n",
    "                batch['int_samples'], \n",
    "                batch['target_row'],\n",
    "                batch['int_mask'],\n",
    "                int_node_idx=batch.get('int_node_idx', None)\n",
    "            )\n",
    "            \n",
    "            if len(output) == 3:\n",
    "                p_delta, p_adj_logits, _ = output\n",
    "            else:\n",
    "                p_delta, p_adj_logits = output\n",
    "                \n",
    "            loss, items = causal_loss_fn(p_delta, batch['delta'], p_adj_logits, batch['adj'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Logging\n",
    "            if step % val_freq == 0 or step == 1:\n",
    "                shd_val = compute_shd(p_adj_logits, batch['adj'])\n",
    "                \n",
    "                history['step'].append(step)\n",
    "                history['loss'].append(loss.item())\n",
    "                history['delta_mse'].append(items['delta'])\n",
    "                history['dag_bce'].append(items['dag'])\n",
    "                history['shd'].append(shd_val)\n",
    "                history['acyclicity'].append(items['h'])\n",
    "                \n",
    "                rprint(f\"Step {step:04d} | Loss: {loss.item():.3f} | MSE: {items['delta']:.3f} | SHD: {shd_val:.1f} | H: {items['h']:.1e}\")\n",
    "        \n",
    "        # Save Checkpoint\n",
    "        torch.save(model.state_dict(), f\"checkpoints/{model_name.replace(' ', '_')}_final.pt\")\n",
    "        \n",
    "        # Save Logs\n",
    "        with open(f\"logs/{model_name.replace(' ', '_')}_log.json\", 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "            \n",
    "        rprint(f\"[bold green]Finished {model_name}. Checkpoint saved.[/bold green]\")\n",
    "        return history\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        rprint(\"[yellow]Training interrupted.[/yellow]\")\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "train_a_md",
   "source": [
    "## 9. Training Model A (Asymmetric Baseline)\n",
    "\n",
    "Standard baseline with shared backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "train_a_code",
   "source": [
    "# Initialize\n",
    "model_a = ModelA_Baseline(num_nodes=50, d_model=128)\n",
    "\n",
    "# Train (20-50 nodes curriculum)\n",
    "hist_a = train_model(model_a, \"Model A Baseline\", steps=3000, val_freq=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "train_b_md",
   "source": [
    "## 10. Training Model B (Variable Experts)\n",
    "\n",
    "Node-specific MLPs for unrelated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "train_b_code",
   "source": [
    "model_b = ModelB_Experts(num_nodes=50, d_model=128)\n",
    "hist_b = train_model(model_b, \"Model B Experts\", steps=3000, val_freq=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "train_c_md",
   "source": [
    "## 11. Training Model C (Gumbel Sparsity)\n",
    "\n",
    "Differentiable discrete structure search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "train_c_code",
   "source": [
    "model_c = ModelC_Sparsity(num_nodes=50, d_model=128)\n",
    "# Optional: Anneal temperature self.tau during training if desired\n",
    "hist_c = train_model(model_c, \"Model C Gumbel\", steps=3000, val_freq=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "train_d_md",
   "source": [
    "## 12. Training Model D (Masked Attention)\n",
    "\n",
    "Structural bias injection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "train_d_code",
   "source": [
    "model_d = ModelD_Masked(num_nodes=50, d_model=128)\n",
    "hist_d = train_model(model_d, \"Model D Masked\", steps=3000, val_freq=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "train_e_md",
   "source": [
    "## 13. Training Model E (Hyper-Network)\n",
    "\n",
    "Context-aware weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "train_e_code",
   "source": [
    "model_e = ModelE_HyperNet(num_nodes=50, d_model=128)\n",
    "hist_e = train_model(model_e, \"Model E HyperNet\", steps=3000, val_freq=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "results_md",
   "source": [
    "## 14. Comparative Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "plot_code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot SHD\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, hist in [(\"Model A\", hist_a), (\"Model B\", hist_b), (\"Model C\", hist_c), (\"Model D\", hist_d), (\"Model E\", hist_e)]:\n",
    "    if 'shd' in hist:\n",
    "        plt.plot(hist['step'], hist['shd'], label=name)\n",
    "plt.title(\"Structural Hamming Distance (Lower is Better)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"SHD\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot MSE\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, hist in [(\"Model A\", hist_a), (\"Model B\", hist_b), (\"Model C\", hist_c), (\"Model D\", hist_d), (\"Model E\", hist_e)]:\n",
    "    if 'delta_mse' in hist:\n",
    "        plt.plot(hist['step'], hist['delta_mse'], label=name)\n",
    "plt.title(\"Delta Prediction MSE (Lower is Better)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}