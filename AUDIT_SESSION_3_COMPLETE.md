# Comprehensive Code Audit Summary - All 15 Issues

## Executive Summary

**Total Issues Found: 15**
- **Previously Fixed (Session 1-2): 13** ‚úÖ All verified and working
- **Newly Discovered (Session 3): 2** üî¥ CRITICAL - Prevent structure learning

---

## Quick Status Dashboard

| Phase | Issues | Status | Severity |
|-------|--------|--------|----------|
| Optimizer & Scheduling | 8 | ‚úÖ FIXED | Medium |
| DDP & Sync | 5 | ‚úÖ FIXED | Medium |
| Data & Model | **2** | üî¥ BROKEN | **CRITICAL** |

---

## The Two Game-Changing Issues

### Why the model can't learn structure:

**Issue #15** (MOST CRITICAL):
```python
# Line 463 of CausalTransformer.py - CURRENT (BROKEN)
dummy_adj = torch.zeros(B, N, N, device=base_samples.device)
return deltas_final, logits_final, dummy_adj, None, total_aux
                       ^^^^^^^^^^               ^^^^^^^^
                    (computed correctly)      (thrown away!)
```

The model computes real structure predictions but returns all-zeros to the loss function. Loss function can't compute gradients on zeros. Structure head never learns.

**Issue #14** (HIGH PRIORITY):
```python
# CausalTransformer.py line 382
def forward(self, base_samples, int_samples, target_row, int_mask, int_node_idx=None, ...):
    # ... 80 lines ...
    # int_node_idx is NEVER used
    # Should embed it: instr = int_embedding(int_node_idx) 
    # and concatenate to encoder input
```

Model receives which node was intervened but ignores it. This is like training a physics model on intervention experiments but hiding the intervention labels from the model.

---

## All 15 Issues: Complete Inventory

### Batch 1: Optimizer Issues (Issues 1-2) ‚úÖ

| # | Title | Fix | Impact |
|---|-------|-----|--------|
| 1 | No weight_decay | Added `weight_decay=1e-4` | Structural sparsity |
| 2 | Grad clip too tight | Increased 1.0 ‚Üí 10.0 | Gradient starvation fix |

### Batch 2: Loss & Curriculum (Issues 3-5) ‚úÖ

| # | Title | Fix | Impact |
|---|-------|-----|--------|
| 3 | lambda_delta=0 starves DAG | Curriculum-linked 100‚Üí1 | Dynamic difficulty |
| 4 | Router not reinitialized | Fresh init with broadcast | Prevent expert collapse |
| 5 | pos_weight clamped too tight | Relaxed 20‚Üí100 range | Sparse graph support |

### Batch 3: Training Loop (Issues 6-8) ‚úÖ

| # | Title | Fix | Impact |
|---|-------|-----|--------|
| 6 | Validation cache ignores density | Check both max_vars & edge_prob | Cache invalidation |
| 7 | Scheduler per-epoch only | Moved to per-batch | Effective warmup |
| 8 | AdamW parameters implicit | Made explicit (lr, weight_decay) | Reproducibility |

### Batch 4: GPU & Synchronization (Issues 9-11) ‚úÖ

| # | Title | Fix | Impact |
|---|-------|-----|--------|
| 9 | Missing dtype in loss | Added `dtype=pred_adj.dtype` | Float/double consistency |
| 10 | Router not synced across ranks | Added broadcast after init | DDP consistency |
| 11 | Metrics not reduced in DDP | Added dist.all_reduce() calls | Curriculum agreement |

### Batch 5: Stability Guards (Issues 12-13) ‚úÖ

| # | Title | Fix | Impact |
|---|-------|-----|--------|
| 12 | NaN loss loses gradients | Added `requires_grad=True` | Gradient preservation |
| 13 | Stale validation cache | Proper invalidation checks | Cache correctness |

### **Batch 6: Data & Model - NEWLY FOUND** üî¥

| # | Title | Fix Needed | Impact |
|---|-------|-----------|--------|
| **14** | **int_node_idx unused** | **Embed and use in forward** | **No intervention awareness** |
| **15** | **Dummy zeros override logits** | **Return logits_final not dummy_adj** | **Structure learning disabled** |

---

## Issue #15: The Structure Learning Blocker

### What's Happening Now (BROKEN):

```python
# CausalTransformer.py lines 457-468
# Returning Dummy Logits/Adj for API compatibility with main.py metrics
if len(base_samples.shape) == 3:
    B, S, N = base_samples.shape
else:
    B, N = base_samples.shape
    
dummy_adj = torch.zeros(B, N, N, device=base_samples.device)

# Total Aux Loss
total_aux = aux_1 + aux_2 + aux_3

return deltas_final, logits_final, dummy_adj, None, total_aux
                     ^^^^^^^^^^^  ^^^^^^^^
                     (Real!)      (Fake!)
```

### Consequences:

1. **Loss Computation** (main.py:487-498):
```python
deltas, logits, adj, _, aux_loss = model(...)  # logits=zeros
loss, items = causal_loss_fn(
    deltas, batch['delta'], 
    logits,                  # ‚Üê ALL ZEROS
    batch['adj'],
    lambda_delta=current_lambda_delta,
    lambda_dag=args.lambda_dag,  # ‚Üê Even if non-zero, loss_dag input is zeros!
```

2. **Gradient Flow**:
```
loss_dag = BCE(zeros, true_adj)
        ‚Üì
grad_dag ‚âà constant (independent of predictions)
        ‚Üì
DAG head doesn't receive meaningful gradients
```

3. **Metrics** (main.py:532-535):
```python
shd = compute_shd(logits, batch['adj'])   # shd(zeros, data) = constant
f1 = compute_f1(logits, batch['adj'])     # f1(zeros, data) = constant
# Both metrics stuck at same value every batch!
```

### Verification Test:

In your next training run, check if:
- `shd` metric stays EXACTLY THE SAME for 100+ batches ‚Üí Issue #15
- `f1` metric never improves ‚Üí Issue #15
- `loss_dag` component is always same value ‚Üí Issue #15

### The Fix (One Line):

```python
# BEFORE (Line 468):
return deltas_final, logits_final, dummy_adj, None, total_aux

# AFTER:
return deltas_final, logits_final, logits_final, None, total_aux
#                                  ^^^^^^^^^^^^
#                                  Use real predictions!
```

That's it. The variable `logits_final` is already computed correctly by Pass 3 of the model.

---

## Issue #14: Missing Intervention Awareness

### What's Happening Now (BROKEN):

```python
# CausalTransformer.py:382
def forward(self, base_samples, int_samples, target_row, int_mask, int_node_idx=None, mcm_mask=None):
```

The model receives:
- `base_samples`: Base observational values 
- `int_samples`: Intervened values (which node was set to what)
- `int_mask`: Binary mask of which nodes were intervened
- `int_node_idx`: **Which specific node was intervened** ‚Üê NOT USED

### Consequences:

1. **Loss of Information**:
```python
# Data provides this:
int_node_idx = [5, 5, 12, 12, 3, ...]  # Which node was intervened
# But model sees:
# (nothing - parameter ignored)
```

2. **Model Can't Distinguish**:
- Sample 1: Intervened on node 5 with value +10 ‚Üí Expects delta for X5
- Sample 2: Intervened on node 7 with value +10 ‚Üí Expects delta for X7
- **Model**: "Both are +10 interventions, predict same delta" ‚úó

3. **Comparison to Original Code** (ISD-CP.ipynb:965):
```python
# NOTEBOOK (WORKING)
instr = self.int_embedding(int_node_idx)  # Embed which node
x = self.encoder(...)
x = torch.cat([x, instr], dim=-1)  # Condition on intervention

# CURRENT CODE (BROKEN)
x = self.encoder(...)
# instr is never created or used
```

### The Fix (10-15 lines):

1. Add embedding layer in `__init__`:
```python
self.int_embedding = nn.Embedding(num_nodes, d_model)
```

2. Use it in `_forward_pass`:
```python
# After encoder output
if int_node_idx is not None:
    instr = self.int_embedding(int_node_idx)  # (B, S) ‚Üí (B, S, D)
    x = x + instr  # Add intervention signal
```

---

## Training Impact: What You're Observing

### Physics Head (Deltas) ‚úÖ
- Learns OK because it only needs the intervention magnitude
- Doesn't need to know WHICH node was intervened

### Structure Head (DAG) üî¥
- Receives all-zero predictions from model (Issue #15)
- Loss function can't compute meaningful gradients
- Metrics stuck at constant values

### Together üî¥
- Model learns "predict average delta" (physics)
- Model never learns "which nodes cause which effects" (structure)
- Curriculum progression based on fake metrics (all-zero predictions)

---

## Verification Checklist Before You Train

- [ ] Line 468 of CausalTransformer.py: Check if returns `dummy_adj` or `logits_final`
- [ ] Line 382 of CausalTransformer.py: Check if `int_node_idx` is used anywhere
- [ ] Search for "int_embedding" in CausalTransformer.py: Should exist
- [ ] Training logs should show: SHD metric improving, F1 improving (not constant)

---

## Why These Bugs Weren't Caught

1. **Issue #15**: Looks like intentional dummy return (has comment "for API compatibility")
2. **Issue #14**: Parameter accepted but marked optional with default None
3. Both are **silent failures** - no errors thrown, just wrong results

---

## Next Steps

1. **Fix #15 immediately**: Change 1 line (line 468)
2. **Fix #14 after**: Add ~15 lines (embedding + concatenation)
3. **Test**: Run `python main.py --dry_run` to verify forward pass
4. **Validate**: Check that structure metrics now change with batches
5. **Train**: Run full training with logging

---

## Reference: Issue Matrix

```
Priority | Issue | Type | Lines | Complexity | Verified
---------|-------|------|-------|-----------|----------
URGENT   | #15   | Logic| 468   | 1 change  | Yes
HIGH     | #14   | Logic| 382   | 15 lines  | Yes
---------|-------|------|-------|-----------|----------
DONE     | 1-13  | Mixed| Multi | Various   | Yes
```

---

## Questions to Validate After Fixes

1. Do SHD/F1 metrics now **change per batch** instead of staying constant?
2. Does `loss_dag` component have **non-zero gradients**?
3. Can you observe **edge discovery** in sample predictions?
4. Does **curriculum progression** accelerate (since metrics are now real)?

If all yes ‚Üí Issues fixed successfully ‚úÖ

